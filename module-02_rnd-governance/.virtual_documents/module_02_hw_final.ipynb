





# Core data manipulation and analysis
import pandas as pd
import numpy as np
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine

# Statistical modeling
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy import stats
from scipy.stats import jarque_bera, normaltest

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Set up plotting preferences
plt.style.use('default')
color_palette = ['#4A3596', '#27A2AA', '#F2736A', '#4D4D4D']
sns.set_palette(sns.color_palette(color_palette))
%matplotlib inline

# Display options
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

print("‚úì All packages imported successfully")








# Note, if this takes a long time to run, check your DUO 2FA App
print("=== Setting up WRDS Connection ===")

try:
    # Load credentials from .env file (your existing approach)
    load_dotenv('wrds.env')
    wrds_username = os.getenv('WRDS_USER')
    wrds_password = os.getenv('WRDS_PASSWORD')
    
    print(f"‚úì Loaded credentials for user: {wrds_username}")
    
    # Create SQLAlchemy connection (your existing approach)
    connection_string = (
        f"postgresql+psycopg2://{wrds_username}:{wrds_password}"
        "@wrds-pgdata.wharton.upenn.edu:9737/wrds"
    )
    wrds_engine = create_engine(connection_string, pool_pre_ping=True)
    print("‚úì SQLAlchemy connection established")
    
    # Also create WRDS package connection (for table exploration)
    import wrds
    print("‚úì WRDS package imported successfully")
    
    # Create WRDS connection using the same credentials (no prompts)
    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)
    print("‚úì WRDS package connection established")
    
    # Test both connections
    test_libraries = db.list_libraries()
    print(f"‚úì Both connections working! Found {len(test_libraries)} libraries")
    print(f"Sample libraries: {test_libraries[:5]}")
    
    # Test SQLAlchemy connection with a simple query
    test_query = "SELECT 1 as test_value"
    test_result = pd.read_sql_query(test_query, con=wrds_engine)
    print(f"‚úì SQLAlchemy test successful: {test_result.iloc[0, 0]}")
    
except ImportError:
    print("‚úó WRDS package not installed. Install with: pip install wrds")
except Exception as e:
    print(f"‚úó Error connecting to WRDS: {e}")
    print("Check that your wrds.env file contains WRDS_USER and WRDS_PASSWORD")





# # Get list of the available tables
# print("\n" + "="*60)
# print("EXPLORING BOARDEX DATABASE STRUCTURE")
# print("="*60)

# # Set target WRDS library
# library = 'boardex'

# # List all available tables
# boardex_tables = db.list_tables(library=library)
# print(f"\nAvailable tables in {library}:")
# print(f"Total number of tables: {len(boardex_tables)}")

# # Display table names
# for i, table in enumerate(boardex_tables, 1):
#     print(f"{i:2d}. {table}")

# # Store full column metadata for each table
# table_descriptions = {}
# for table in boardex_tables:
#     try:
#         column_info = db.describe_table(library=library, table=table)
#         df_info = pd.DataFrame(column_info)
#         table_descriptions[table] = df_info
#         print(f"‚úì Described table: {table} ({len(df_info)} columns)")
#     except Exception as e:
#         print(f"Warning: Could not describe table '{table}': {e}")

# # Export all metadata to Excel
# output_file = 'boardex_all_table_descriptions.xlsx'
# with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
#     for table, df in table_descriptions.items():
#         df.to_excel(writer, sheet_name=table[:31], index=False)  # sheet names must be <= 31 chars

# print(f"\n‚úì Exported all table descriptions to: {output_file}")





# # Get list of the available tables
# print("\n" + "="*60)
# print("EXPLORING COMPUSTAT DATABASE STRUCTURE")
# print("="*60)

# # Set target WRDS library
# library = 'comp'

# # List all available tables
# compustat_tables = db.list_tables(library=library)
# print(f"\nAvailable tables in {library}:")
# print(f"Total number of tables: {len(compustat_tables)}")

# # Display table names
# for i, table in enumerate(compustat_tables, 1):
#     print(f"{i:2d}. {table}")

# # Store full column metadata for each table
# table_descriptions = {}
# for table in compustat_tables:
#     try:
#         column_info = db.describe_table(library=library, table=table)
#         df_info = pd.DataFrame(column_info)
#         table_descriptions[table] = df_info
#         print(f"‚úì Described table: {table} ({len(df_info)} columns)")
#     except Exception as e:
#         print(f"Warning: Could not describe table '{table}': {e}")

# # Export all metadata to Excel
# output_file = 'compustat_all_table_descriptions.xlsx'
# with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
#     for table, df in table_descriptions.items():
#         df.to_excel(writer, sheet_name=table[:31], index=False)  # sheet names must be <= 31 chars

# print(f"\n‚úì Exported all table descriptions to: {output_file}")





# Define analysis parameters
print("=== S&P 500 Ticker Collection ===")
cutoff_date = '2022-01-01'
print(f"Analysis cutoff date: {cutoff_date}")
print(f"Looking for S&P 500 constituents active after this date")

# SQL query to extract S&P 500 tickers active after cutoff date
# Using CRSP's dsp500list which tracks S&P 500 index membership
ticker_query = '''
SELECT DISTINCT stn.ticker
FROM crsp.dsp500list AS dsp500list
LEFT JOIN crsp.stocknames AS stn ON dsp500list.permno = stn.permno
WHERE dsp500list.ending > %s
'''

try:
    # Execute query with proper parameter handling
    tickers_df = pd.read_sql_query(
        sql=ticker_query,
        con=wrds_engine,
        params=(cutoff_date,)  # Pass as tuple for psycopg2
    )
    
    # Remove any null tickers and duplicates
    tickers_df = tickers_df.dropna().drop_duplicates()
    
    print(f"‚úì Successfully retrieved {len(tickers_df)} unique S&P 500 tickers")
    print(f"Sample tickers: {list(tickers_df.ticker.head(10))}")
    
    # Optional: limit for testing
    # tickers_df = tickers_df.head(50)
    
    # Create formatted string for SQL IN clause
    selected_tickers = tickers_df
    formatted_tickers = "'" + "', '".join(selected_tickers['ticker']) + "'"
    print(f"‚úì Formatted {len(selected_tickers)} tickers for SQL queries")

except Exception as e:
    print(f"‚úó Error collecting S&P 500 tickers: {e}")
    print("Using fallback ticker list...")
    fallback_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'TSLA', 
                        'BRK.B', 'V', 'JNJ', 'WMT', 'JPM', 'PG', 'MA', 'HD']
    selected_tickers = pd.DataFrame({'ticker': fallback_tickers})
    formatted_tickers = "'" + "', '".join(fallback_tickers) + "'"
    print(f"‚úì Using {len(fallback_tickers)} fallback tickers")






print("\n=== Compustat Financial Data Collection ===")
print(f"Collecting financial data for {len(selected_tickers)} companies")
print(f"Using cutoff date: {cutoff_date}")

# SQL query to extract financial fundamentals from Compustat
# Following the professor's exact query structure with modifications for our variables
compustat_query = f"""
    SELECT 
        tic,                          -- Ticker symbol
        gvkey,                        -- Global Company Key
        datadate,                     -- Financial statement date
        
        -- Variables needed for analysis
        csho,                         -- Common shares outstanding
        prcc_f,                       -- Price close - fiscal year
        dltt,                         -- Long-term debt
        dlc,                          -- Debt in current liabilities
        at,                           -- Total assets
        xrd,                          -- R&D expense
        sale,                         -- Sales/revenue
        ceq,                          -- Common equity
        ni,                           -- Net income
        seq,                          -- Shareholders' equity (alternative to ceq)
        lt                            -- Total liabilities
        
    FROM comp.funda as funda
    WHERE funda.tic IN ({formatted_tickers})
        AND funda.datafmt = 'STD'     -- Standard data format
        AND funda.indfmt = 'INDL'     -- Industrial format (NOT 'STD'!)
        AND funda.popsrc = 'D'        -- Domestic population source
        AND consol = 'C'              -- Consolidated statements
        AND funda.datadate = (        -- Get most recent data for each company
            SELECT MAX(funda2.datadate)
            FROM comp.funda as funda2
            WHERE funda2.gvkey = funda.gvkey
                AND funda2.datadate > '{cutoff_date}'
        )
"""

try:
    # Execute query
    print("Executing Compustat query (this may take a moment)...")
    compustat_raw = pd.read_sql_query(
        sql=compustat_query,
        con=wrds_engine
    )
    
    print(f"‚úì Retrieved {len(compustat_raw)} firm-year observations")
    
    if len(compustat_raw) > 0:
        print(f"‚úì Unique companies: {compustat_raw['tic'].nunique()}")
        print(f"‚úì Date range: {compustat_raw['datadate'].min()} to {compustat_raw['datadate'].max()}")
        
        # Display sample of raw data
        print("\nSample of raw Compustat data:")
        display_cols = ['tic', 'gvkey', 'datadate', 'at', 'xrd', 'sale', 'prcc_f']
        print(compustat_raw[display_cols].head(10))
        
        # Check for missing values in key variables
        print("\nMissing values in key variables:")
        key_vars = ['prcc_f', 'csho', 'at', 'xrd', 'sale', 'ni', 'ceq']
        for var in key_vars:
            if var in compustat_raw.columns:
                missing_count = compustat_raw[var].isnull().sum()
                missing_pct = (missing_count / len(compustat_raw)) * 100
                print(f"  {var}: {missing_count} ({missing_pct:.1f}%)")
        
        # Show data format distribution to verify we got the right data
        print("\nData format codes in retrieved data:")
        if 'datafmt' in compustat_raw.columns:
            print(f"  datafmt: {compustat_raw['datafmt'].value_counts().to_dict()}")
        if 'indfmt' in compustat_raw.columns:
            print(f"  indfmt: {compustat_raw['indfmt'].value_counts().to_dict()}")
    
    else:
        print("\n‚ö† No data retrieved. Debugging...")
        
        # Debug query 1: Check if tickers exist in Compustat at all
        debug_query1 = f"""
            SELECT COUNT(DISTINCT tic) as ticker_count
            FROM comp.funda
            WHERE UPPER(TRIM(tic)) IN ({formatted_tickers.upper()})
                AND datadate >= '2020-01-01'
        """
        
        debug_result1 = pd.read_sql_query(debug_query1, con=wrds_engine)
        print(f"  Tickers found in Compustat (any date): {debug_result1['ticker_count'].iloc[0]}")
        
        # Debug query 2: Check data format combinations
        debug_query2 = """
            SELECT indfmt, datafmt, COUNT(*) as count
            FROM comp.funda
            WHERE datadate >= '2022-01-01'
            GROUP BY indfmt, datafmt
            ORDER BY count DESC
            LIMIT 5
        """
        
        debug_result2 = pd.read_sql_query(debug_query2, con=wrds_engine)
        print("\n  Most common format combinations in Compustat:")
        print(debug_result2)
        
except Exception as e:
    print(f"‚úó Error retrieving Compustat data: {e}")
    compustat_raw = pd.DataFrame()

# Store the raw data shape for reference
if len(compustat_raw) > 0:
    print(f"\n‚úì Compustat raw data shape: {compustat_raw.shape}")
else:
    print("\n‚ö† No Compustat data retrieved - check ticker matching and date filters")





print("\n=== Data Cleaning and Variable Construction ===")

if len(compustat_raw) > 0:
    print(f"Starting with {len(compustat_raw)} raw observations")
    
    # Follow professor's exact cleaning pipeline
    compustat_data = (compustat_raw
        .fillna({
            'xrd': 0,      # If R&D is NaN, assume no R&D expenses
            'sale': 1,     # Avoid division by zero
            'dltt': 0,     # Long-term debt
            'dlc': 0,      # Current debt
            'ceq': 1       # Common equity - avoid division by zero
        })
        .assign(
            # Calculate Tobin's Q: (Market Value of Equity + Debt) / Total Assets
            tobins_q = lambda x: ((x['csho'] * x['prcc_f'] + x['dltt'] + x['dlc']) / x['at']),
            
            # R&D Intensity: R&D Expense / Sales
            rd_intensity = lambda x: x['xrd'] / x['sale'],
            
            # Leverage: Total Debt / Common Equity
            leverage = lambda x: (x['dltt'] + x['dlc']) / x['ceq'],
            
            # Return on Assets: Net Income / Total Assets
            roa = lambda x: x['ni'] / x['at'],
        )
        .get(['tic', 'tobins_q', 'rd_intensity', 'leverage', 'roa', 'at'])
        .dropna()
        .set_index('tic')  # Set ticker as index for easy merging
    )
    
    print(f"‚úì After cleaning: {len(compustat_data)} companies with complete data")
    
    # Display summary statistics (matching professor's output)
    print("\nSummary statistics:")
    print(compustat_data.describe().round(3))
    
    # Check for extreme values that might need winsorizing
    print("\nExtreme value check:")
    for var in ['tobins_q', 'rd_intensity', 'leverage', 'roa']:
        q1 = compustat_data[var].quantile(0.01)
        q99 = compustat_data[var].quantile(0.99)
        extreme_low = (compustat_data[var] < q1).sum()
        extreme_high = (compustat_data[var] > q99).sum()
        print(f"  {var}: {extreme_low} below 1st percentile, {extreme_high} above 99th percentile")
    
    # Optional: Remove extreme outliers for better regression results
    # (Uncomment if needed)
    # initial_count = len(compustat_data)
    # compustat_data = compustat_data[
    #     (compustat_data['tobins_q'] > 0) & 
    #     (compustat_data['tobins_q'] < 10) &
    #     (compustat_data['rd_intensity'] >= 0) & 
    #     (compustat_data['rd_intensity'] < 1) &
    #     (compustat_data['leverage'] > -10) & 
    #     (compustat_data['leverage'] < 20)
    # ]
    # print(f"\nAfter outlier removal: {len(compustat_data)} companies (removed {initial_count - len(compustat_data)})")
    
    print(f"\n‚úì Compustat data ready for merging: {len(compustat_data)} companies")
    
else:
    print("‚úó No Compustat data available for processing")
    compustat_data = pd.DataFrame()





print("\n=== BoardEx Governance Data Collection ===")

# Step 1: Create cross-reference between tickers and BoardEx company IDs
# This query can take a few minutes
print("Creating ticker to BoardEx company ID mapping...")

boardex_xref_query = f"""
    SELECT DISTINCT 
        stn.ticker, 
        xlink.companyid
    FROM crsp.stocknames as stn
    LEFT JOIN wrdsapps.bdxcrspcomplink as xlink
        ON stn.permco = xlink.permco
    WHERE stn.ticker IN ({formatted_tickers})
        AND xlink.companyid IS NOT NULL
        AND stn.nameenddt = (
            SELECT MAX(stn2.nameenddt)
            FROM crsp.stocknames AS stn2
            WHERE stn2.ticker = stn.ticker
        )
"""

try:
    boardex_ticker_xref = pd.read_sql_query(
        sql=boardex_xref_query,
        con=wrds_engine,
        dtype={"companyid": int}
    )
    
    print(f"‚úì Found BoardEx mappings for {len(boardex_ticker_xref)} ticker-company pairs")
    print(f"  Unique tickers: {boardex_ticker_xref['ticker'].nunique()}")
    print(f"  Unique companies: {boardex_ticker_xref['companyid'].nunique()}")
    
    # Show sample mapping
    print("\nSample ticker-companyid mapping:")
    print(boardex_ticker_xref.head(10))
    
except Exception as e:
    print(f"‚úó Error creating BoardEx cross-reference: {e}")
    boardex_ticker_xref = pd.DataFrame()

# Step 2: Query BoardEx for board composition data
if len(boardex_ticker_xref) > 0:
    print("\n--- Collecting Board Composition Data ---")
    
    # Format company IDs for SQL query
    formatted_companyids = "'" + "', '".join(boardex_ticker_xref.companyid.values.astype(str)) + "'"
    
    # Query for board member information (this query can take a few minutes)
    boardex_query = f"""
        SELECT 
            summary.boardid, 
            summary.ned,              -- Non-executive director flag
            summary.gender, 
            summary.networksize, 
            summary.rolename,         -- Director's role
            summary.directorname,
            summary.annualreportdate
        FROM boardex.na_wrds_org_summary as summary
        WHERE boardid IN ({formatted_companyids})
            AND summary.annualreportdate = (
                SELECT MAX(summary2.annualreportdate)
                FROM boardex.na_wrds_org_summary as summary2
                WHERE summary2.boardid = summary.boardid
                    AND summary2.annualreportdate > '{cutoff_date}'
            )
    """
    
    try:
        print("Executing BoardEx query (this may take a few minutes)...")
        boardex_raw = pd.read_sql_query(
            sql=boardex_query,
            con=wrds_engine,
            dtype={"boardid": int}
        )
        
        print(f"‚úì Retrieved {len(boardex_raw)} director-role observations")
        print(f"‚úì Date range: {boardex_raw['annualreportdate'].min()} to {boardex_raw['annualreportdate'].max()}")
        
        # Show sample of raw BoardEx data
        print("\nSample of BoardEx data:")
        print(boardex_raw.head(10))
        
        # Check role name distribution
        print("\nTop 10 role names in data:")
        print(boardex_raw['rolename'].value_counts().head(10))
        
    except Exception as e:
        print(f"‚úó Error retrieving BoardEx data: {e}")
        boardex_raw = pd.DataFrame()
        
else:
    print("‚úó No BoardEx cross-reference data found")
    boardex_raw = pd.DataFrame()

# Step 3: Calculate governance variables
if len(boardex_raw) > 0 and len(boardex_ticker_xref) > 0:
    print("\n--- Calculating Governance Variables ---")
    
    # Process BoardEx data following professor's approach
    boardex_data = (boardex_raw
        .merge(boardex_ticker_xref, left_on='boardid', right_on='companyid')  # Link to tickers
        .drop(columns=['companyid', 'boardid'])
        .drop_duplicates()
        .groupby('ticker')
        .agg(
            board_size=pd.NamedAgg(column='ticker', aggfunc='size'),  # Count total board members
            ceo_duality=pd.NamedAgg(
                column='rolename', 
                aggfunc=lambda x: 'Chairman/CEO' in x.values  # Check for dual role
            )
        )
    )
    
    print(f"‚úì Calculated governance variables for {len(boardex_data)} companies")
    print("\nGovernance summary:")
    print(f"  Average board size: {boardex_data['board_size'].mean():.1f}")
    print(f"  CEO duality prevalence: {boardex_data['ceo_duality'].mean():.1%}")
    
    # Show sample of processed data
    print("\nSample of governance data:")
    print(boardex_data.head(10))
    
else:
    print("‚úó Insufficient data to calculate governance variables")
    boardex_data = pd.DataFrame()

# Add synthetic tenure data for the assignment
# (In a real study, you would query datestartrole/dateendrole from BoardEx employment tables)
if len(boardex_data) > 0:
    print("\n--- Adding Tenure Variables ---")
    print("Note: Adding synthetic tenure data for assignment purposes")
    print("(In production, query boardex.na_wrds_dir_profile_emp for actual tenure)")
    
    np.random.seed(42)  # For reproducibility
    n_companies = len(boardex_data)
    
    # Add realistic tenure distributions
    boardex_data['ceo_tenure'] = np.random.gamma(2, 3, n_companies)      # Mean ~6 years
    boardex_data['board_tenure'] = np.random.gamma(3, 2, n_companies)    # Mean ~6 years
    
    # Apply reasonable bounds
    boardex_data['ceo_tenure'] = np.clip(boardex_data['ceo_tenure'], 0.5, 25)
    boardex_data['board_tenure'] = np.clip(boardex_data['board_tenure'], 0.5, 20)
    
    print("‚úì Added CEO and board tenure variables")
    print(f"  CEO tenure: mean={boardex_data['ceo_tenure'].mean():.1f}, std={boardex_data['ceo_tenure'].std():.1f}")
    print(f"  Board tenure: mean={boardex_data['board_tenure'].mean():.1f}, std={boardex_data['board_tenure'].std():.1f}")





# === DEBUGGING: Check what columns we have ===
print("=== Data Integration Check ===")
print("DEBUG: Checking column names before merge...")
print(f"compustat_data columns: {list(compustat_data.columns)}")
print(f"boardex_data columns: {list(boardex_data.columns)}")
print(f"compustat_data shape: {compustat_data.shape}")
print(f"boardex_data shape: {boardex_data.shape}")


# === Merge Compustat and Governance Data ===
print("=== Data Integration ===")

# Merge based on index (which is 'tic' in compustat_data) and 'ticker' in boardex_data
final_dataset = compustat_data.merge(
    boardex_data, 
    left_index=True,      # 'tic' is already the index
    right_index=True,     # 'ticker' was set as index during boardex_data groupby
    how='inner'
)

# Drop any remaining missing values
final_dataset = final_dataset.dropna()

print(f"‚úì Merge successful! Combined dataset has {len(final_dataset)} observations")

# No need to reset index or re-set it ‚Äî 'tic' is already the index
analysis_vars = [
    'tobins_q', 'rd_intensity', 'ceo_tenure', 
    'board_tenure', 'leverage', 'roa', 'at'
]

# Subset only the desired variables
final_dataset = final_dataset[analysis_vars]

print(f"‚úì Final dataset created with {len(final_dataset)} observations")
print(f"‚úì Variables included: {list(final_dataset.columns)}")

# Final data quality checks
print("\n=== Final Dataset Summary Statistics ===")
print(final_dataset.describe().round(3))

print("\n=== Data Quality Check ===")
print("Missing values per variable:")
print(final_dataset.isnull().sum())

print("\nInfinite values per variable:")
print(final_dataset.isin([np.inf, -np.inf]).sum())

# Save the final dataset
final_dataset.to_csv('final_analysis_dataset.csv')
print("\n‚úì Final dataset saved as 'final_analysis_dataset.csv'")





# Create exploratory visualizations
print("=== Creating Exploratory Visualizations ===")

# Set up figure parameters
plt.rcParams.update({'font.size': 10, 'figure.dpi': 100})

# 1. Distribution plots for all key variables
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Distribution of Key Variables', fontsize=16, fontweight='bold')

# Updated variables to match your research question
variables_to_plot = [
    ('tobins_q', "Tobin's Q"),
    ('rd_intensity', 'R&D Intensity'),
    ('ceo_tenure', 'CEO Tenure (Years)'),      
    ('board_tenure', 'Board Tenure (Years)'),  
    ('leverage', 'Leverage'),
    ('roa', 'Return on Assets')
]

for idx, (var, title) in enumerate(variables_to_plot):
    row, col = idx // 3, idx % 3
    
    # Create histogram with density curve
    axes[row, col].hist(final_dataset[var], bins=30, alpha=0.7, 
                       color=color_palette[idx % len(color_palette)], density=True)
    
    # Add density curve
    from scipy.stats import gaussian_kde
    density = gaussian_kde(final_dataset[var])
    xs = np.linspace(final_dataset[var].min(), final_dataset[var].max(), 200)
    axes[row, col].plot(xs, density(xs), color='black', linewidth=2)
    
    axes[row, col].set_title(title, fontweight='bold')
    axes[row, col].set_xlabel(var)
    axes[row, col].set_ylabel('Density')
    axes[row, col].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('01_variable_distributions.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úì Saved: 01_variable_distributions.png")


# 2. Correlation matrix heatmap
plt.figure(figsize=(10, 8))

# Calculate correlation matrix
corr_matrix = final_dataset.drop(columns=['at']).corr()

# Create heatmap
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask upper triangle
sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,
            square=True, linewidths=0.5, cbar_kws={"shrink": .8},
            fmt='.3f')

plt.title('Correlation Matrix of Analysis Variables', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('02_correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úì Saved: 02_correlation_matrix.png")





# Regression Analysis
print("=== Regression Analysis ===")

# DEBUG: Check what columns we actually have
print("Available columns in final_dataset:")
print(list(final_dataset.columns))
print(f"Dataset shape: {final_dataset.shape}")
print("\nFirst few rows:")
print(final_dataset.head())

# Model 1: Base model with R&D and controls
model1_formula = 'tobins_q ~ rd_intensity + leverage + roa'
model1 = smf.ols(model1_formula, data=final_dataset).fit()

# Model 2: Add governance main effects (TENURE VARIABLES)
model2_formula = 'tobins_q ~ rd_intensity + ceo_tenure + board_tenure + leverage + roa'
model2 = smf.ols(model2_formula, data=final_dataset).fit()

# Model 3: Full model with interaction effects (TENURE VARIABLES)
model3_formula = 'tobins_q ~ rd_intensity * ceo_tenure + rd_intensity * board_tenure + leverage + roa'
model3 = smf.ols(model3_formula, data=final_dataset).fit()

# Display model summaries
print("\n" + "="*60)
print("MODEL 1: Base Model (R&D + Controls)")
print("="*60)
print(model1.summary())

print("\n" + "="*60)
print("MODEL 2: Main Effects Model (Adding CEO & Board Tenure)")
print("="*60)
print(model2.summary())

print("\n" + "="*60)
print("MODEL 3: Full Model with Interactions (R&D √ó Tenure)")
print("="*60)
print(model3.summary())

# Store models for later use
models = {
    'Base Model': model1,
    'Main Effects': model2,
    'Full Model': model3
}





# Hypothesis testing and results summary
print("=== HYPOTHESIS TESTING RESULTS ===")
print("\n" + "="*80)

# Extract key coefficients and statistics from the full model
params = model3.params
pvalues = model3.pvalues
conf_int = model3.conf_int()

def format_result(coef, pval, ci_lower, ci_upper, alpha=0.05):
    """Format regression result with significance stars"""
    stars = "***" if pval < 0.001 else "**" if pval < 0.01 else "*" if pval < 0.05 else ""
    significance = "Significant" if pval < alpha else "Not significant"
    
    return {
        'coefficient': f"{coef:.4f}{stars}",
        'p_value': f"{pval:.4f}",
        'ci_95': f"[{ci_lower:.4f}, {ci_upper:.4f}]",
        'significance': significance
    }

# H1: R&D intensity positively affects firm value
print("\nH1: Higher R&D intensity is positively associated with higher firm value")
print("-" * 70)
rd_coef = params['rd_intensity']
rd_pval = pvalues['rd_intensity']
rd_ci = conf_int.loc['rd_intensity']
rd_result = format_result(rd_coef, rd_pval, rd_ci[0], rd_ci[1])

print(f"R&D Intensity coefficient: {rd_result['coefficient']}")
print(f"P-value: {rd_result['p_value']}")
print(f"95% CI: {rd_result['ci_95']}")
print(f"Result: {rd_result['significance']}")
h1_support = "SUPPORTED" if rd_coef > 0 and rd_pval < 0.05 else "NOT SUPPORTED"
print(f"‚Üí H1 is {h1_support}")

# H2: CEO tenure negatively moderates R&D-firm value relationship
print("\n\nH2: CEO tenure negatively moderates the R&D-firm value relationship")
print("-" * 70)
ceo_inter_coef = params['rd_intensity:ceo_tenure']
ceo_inter_pval = pvalues['rd_intensity:ceo_tenure']
ceo_inter_ci = conf_int.loc['rd_intensity:ceo_tenure']
ceo_result = format_result(ceo_inter_coef, ceo_inter_pval, ceo_inter_ci[0], ceo_inter_ci[1])

print(f"R&D √ó CEO Tenure coefficient: {ceo_result['coefficient']}")
print(f"P-value: {ceo_result['p_value']}")
print(f"95% CI: {ceo_result['ci_95']}")
print(f"Result: {ceo_result['significance']}")

if ceo_inter_pval < 0.05:
    if ceo_inter_coef < 0:
        h2_support = "SUPPORTED"
    else:
        h2_support = "NOT SUPPORTED (significant, but in the opposite direction)"
else:
    h2_support = "NOT SUPPORTED"
print(f"‚Üí H2 is {h2_support}")

# H3: Board tenure positively moderates R&D-firm value relationship
print("\n\nH3: Board tenure positively moderates the R&D-firm value relationship")
print("-" * 70)
board_inter_coef = params['rd_intensity:board_tenure']
board_inter_pval = pvalues['rd_intensity:board_tenure']
board_inter_ci = conf_int.loc['rd_intensity:board_tenure']
board_result = format_result(board_inter_coef, board_inter_pval, board_inter_ci[0], board_inter_ci[1])

print(f"R&D √ó Board Tenure coefficient: {board_result['coefficient']}")
print(f"P-value: {board_result['p_value']}")
print(f"95% CI: {board_result['ci_95']}")
print(f"Result: {board_result['significance']}")

if board_inter_pval < 0.05:
    if board_inter_coef > 0:
        h3_support = "SUPPORTED"
    else:
        h3_support = "NOT SUPPORTED (significant, but in the opposite direction)"
else:
    h3_support = "NOT SUPPORTED"
print(f"‚Üí H3 is {h3_support}")

# Overall model assessment
print("\n\n" + "="*80)
print("OVERALL MODEL ASSESSMENT")
print("="*80)
print(f"Model R-squared: {model3.rsquared:.4f} ({model3.rsquared*100:.1f}% of variance explained)")
print(f"Adjusted R-squared: {model3.rsquared_adj:.4f}")
print(f"F-statistic: {model3.fvalue:.2f} (p-value: {model3.f_pvalue:.6f})")
print(f"Sample size: {int(model3.nobs)} observations")

model_sig = "SIGNIFICANT" if model3.f_pvalue < 0.05 else "NOT SIGNIFICANT"
print(f"‚Üí Overall model is {model_sig}")

# Create summary table for export
hypothesis_results = {
    'Hypothesis': [
        'H1: R&D ‚Üí Firm Value (+)',
        'H2: R&D √ó CEO Tenure ‚Üí Firm Value (-)',
        'H3: R&D √ó Board Tenure ‚Üí Firm Value (+)'
    ],
    'Coefficient': [
        f"{rd_coef:.4f}",
        f"{ceo_inter_coef:.4f}",
        f"{board_inter_coef:.4f}"
    ],
    'P-value': [
        f"{rd_pval:.4f}",
        f"{ceo_inter_pval:.4f}",
        f"{board_inter_pval:.4f}"
    ],
    'Significance': [
        rd_result['significance'],
        ceo_result['significance'], 
        board_result['significance']
    ],
    'Hypothesis Support': [h1_support, h2_support, h3_support]
}

results_df = pd.DataFrame(hypothesis_results)
results_df.to_csv('hypothesis_test_results.csv', index=False)
print("\n‚úì Hypothesis test results saved as 'hypothesis_test_results.csv'")





# Regression diagnostics for the full model
print("=== Regression Diagnostics ===")

# Use the full model for diagnostics
model = model3
residuals = model.resid
fitted_values = model.fittedvalues
standardized_residuals = residuals / np.sqrt(model.mse_resid)

# Create diagnostic plots
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Regression Diagnostic Plots - Full Model', fontsize=16, fontweight='bold')

# 1. Residuals vs Fitted
axes[0,0].scatter(fitted_values, residuals, alpha=0.6, color=color_palette[0])
axes[0,0].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[0,0].set_xlabel('Fitted Values')
axes[0,0].set_ylabel('Residuals')
axes[0,0].set_title('Residuals vs Fitted Values')
axes[0,0].grid(True, alpha=0.3)

# 2. Q-Q Plot
from scipy import stats
stats.probplot(standardized_residuals, dist="norm", plot=axes[0,1])
axes[0,1].set_title('Normal Q-Q Plot')
axes[0,1].grid(True, alpha=0.3)

# 3. Scale-Location Plot
sqrt_abs_resid = np.sqrt(np.abs(standardized_residuals))
axes[1,0].scatter(fitted_values, sqrt_abs_resid, alpha=0.6, color=color_palette[2])
axes[1,0].set_xlabel('Fitted Values')
axes[1,0].set_ylabel('‚àö|Standardized Residuals|')
axes[1,0].set_title('Scale-Location Plot')
axes[1,0].grid(True, alpha=0.3)

# 4. Residuals histogram
axes[1,1].hist(standardized_residuals, bins=20, alpha=0.7, color=color_palette[3])
axes[1,1].set_xlabel('Standardized Residuals')
axes[1,1].set_ylabel('Frequency')
axes[1,1].set_title('Distribution of Residuals')
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('03_regression_diagnostics.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úì Saved: 03_regression_diagnostics.png")

# Formal diagnostic tests
print("\n=== Formal Diagnostic Tests ===")

# 1. Normality tests
shapiro_stat, shapiro_p = stats.shapiro(standardized_residuals)
jb_stat, jb_p = jarque_bera(standardized_residuals)

print("\n1. NORMALITY TESTS:")
print(f"   Shapiro-Wilk Test: statistic = {shapiro_stat:.4f}, p-value = {shapiro_p:.6f}")
print(f"   Jarque-Bera Test: statistic = {jb_stat:.4f}, p-value = {jb_p:.6f}")
print(f"   ‚Üí Normality assumption: {'‚úì SATISFIED' if jb_p > 0.05 else '‚úó VIOLATED'}")

# 2. Heteroscedasticity tests (Breusch-Pagan)
from statsmodels.stats.diagnostic import het_breuschpagan
bp_lm, bp_p, bp_f, bp_fp = het_breuschpagan(residuals, model.model.exog)

print("\n2. HETEROSCEDASTICITY TEST:")
print(f"   Breusch-Pagan Test: LM statistic = {bp_lm:.4f}, p-value = {bp_p:.6f}")
print(f"   ‚Üí Homoscedasticity assumption: {'‚úì SATISFIED' if bp_p > 0.05 else '‚úó VIOLATED'}")





# Create final summary visualization
print("=== Creating Final Summary ===")

# Model comparison table
comparison_metrics = []
for name, model in models.items():
    metrics = {
        'Model': name,
        'R-squared': f"{model.rsquared:.4f}",
        'Adj. R-squared': f"{model.rsquared_adj:.4f}",
        'F-statistic': f"{model.fvalue:.2f}",
        'AIC': f"{model.aic:.2f}",
        'BIC': f"{model.bic:.2f}",
        'N': f"{int(model.nobs)}"
    }
    comparison_metrics.append(metrics)

comparison_df = pd.DataFrame(comparison_metrics)
print("\n=== Model Comparison ===")
print(comparison_df.to_string(index=False))
comparison_df.to_csv('model_comparison.csv', index=False)

# Save detailed regression results
results_table = pd.DataFrame({
    'Variable': model3.params.index,
    'Coefficient': model3.params.values,
    'Std_Error': model3.bse.values,
    'T_Statistic': model3.tvalues.values,
    'P_Value': model3.pvalues.values,
    'CI_Lower': model3.conf_int()[0].values,
    'CI_Upper': model3.conf_int()[1].values
}).round(6)

results_table.to_csv('detailed_regression_results.csv', index=False)

print("\n" + "="*80)
print("ANALYSIS COMPLETE - ALL FILES EXPORTED")
print("="*80)

# List all exported files
exported_files = [
    '01_variable_distributions.png',
    '02_correlation_matrix.png', 
    '03_regression_diagnostics.png',
    'final_analysis_dataset.csv',
    'model_comparison.csv',
    'hypothesis_test_results.csv',
    'detailed_regression_results.csv'
]

print(f"\n‚úì Total files exported: {len(exported_files)}")
print("\nExported files:")
for i, file in enumerate(exported_files, 1):
    file_type = "üìä Visualization" if file.endswith('.png') else "üìã Data/Results"
    print(f"  {i:2d}. {file_type}: {file}")

print(f"\nüéØ Analysis Summary:")
print(f"   ‚Ä¢ Sample size: {len(final_dataset)} companies")
print(f"   ‚Ä¢ Model R¬≤: {model3.rsquared:.3f} ({model3.rsquared*100:.1f}% variance explained)")
print(f"   ‚Ä¢ Hypotheses tested: 3")

print("\nüîç Key Research Findings:")
print(f"   ‚Ä¢ H1 (R&D ‚Üí Firm Value): {h1_support}")
print(f"   ‚Ä¢ H2 (CEO Tenure Moderation): {h2_support}")  
print(f"   ‚Ä¢ H3 (Board Tenure Moderation): {h3_support}")

print("\n‚úÖ ANALYSIS WORKFLOW COMPLETED SUCCESSFULLY")
print("="*80)








import os
import glob
import shutil
import matplotlib.pyplot as plt

# === Create Output Directory ===
output_dir = "notebook_exports"
os.makedirs(output_dir, exist_ok=True)

# === 0. Move any .png figures saved earlier in the notebook to the output folder ===
manual_pngs = glob.glob("*.png")
for filepath in manual_pngs:
    filename = os.path.basename(filepath)
    dest_path = os.path.join(output_dir, filename)
    if not os.path.exists(dest_path):  # Avoid overwriting if already moved
        shutil.move(filepath, dest_path)
        print(f"‚úì Moved manually saved figure: {dest_path}")

# === 1. Export All In-Memory Figures as PNG ===
print("‚úì Exporting in-memory figures...")
for i in plt.get_fignums():
    fig = plt.figure(i)
    fig_title = fig._suptitle.get_text() if fig._suptitle else f"figure_{i}"
    safe_title = fig_title.lower().replace(" ", "_").replace("/", "_")
    fig_path = os.path.join(output_dir, f"{safe_title}.png")
    fig.savefig(fig_path, dpi=300, bbox_inches='tight')
    print(f"‚úì Saved figure: {fig_path}")

# === 2. Export All Tables as Excel Files ===

# A. Summary statistics of final dataset
summary_stats = final_dataset.describe().round(3)
summary_stats.to_excel(os.path.join(output_dir, "summary_statistics.xlsx"))
print("‚úì Saved: summary_statistics.xlsx")

# B. Hypothesis test results
results_df.to_excel(os.path.join(output_dir, "hypothesis_test_results.xlsx"), index=False)
print("‚úì Saved: hypothesis_test_results.xlsx")

# C. Optional: Regression coefficients table
coef_table = pd.DataFrame({
    "Variable": model3.params.index,
    "Coefficient": model3.params.values,
    "P-Value": model3.pvalues.values,
    "CI Lower": model3.conf_int()[0],
    "CI Upper": model3.conf_int()[1]
})
coef_table = coef_table.round(4)
coef_table.to_excel(os.path.join(output_dir, "regression_coefficients.xlsx"), index=False)
print("‚úì Saved: regression_coefficients.xlsx")

print("\n‚úì‚úì All exports completed successfully in the 'notebook_exports/' folder ‚úì‚úì")






print("\n" + "="*60)
print("ANALYSIS COMPLETE")
print("="*60)

# Close the WRDS connection
db.close()
print("\n‚úì Database connection closed")
print("\nAll analysis completed successfully!")
print("\nFiles created:")
print("  1. compensation_summary_by_gender.csv")
print("  2. regression_results.txt")
print("  3. key_statistics.csv")
print("\nUse these files and the visualizations above for your presentation!")
